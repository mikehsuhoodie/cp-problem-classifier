{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9011ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U accelerate\n",
    "# !pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3374ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John gave Jack a very hard problem. He wrote a...</td>\n",
       "      <td>['math']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Due to the recent popularity of the Deep learn...</td>\n",
       "      <td>['dynamic programming', 'matrices']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bill is a famous mathematician in BubbleLand. ...</td>\n",
       "      <td>['greedy', 'sorting']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The competitors of Bubble Cup X gathered after...</td>\n",
       "      <td>['shortest path', 'graphs', 'binary search']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John has just bought a new car and is planning...</td>\n",
       "      <td>['dynamic programming']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consider an array A with N elements, all being...</td>\n",
       "      <td>['combinatorics', 'number theory', 'math']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The citizens of BubbleLand are celebrating the...</td>\n",
       "      <td>['dynamic programming', 'geometry']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This story is happening in a town named Bubble...</td>\n",
       "      <td>['trees', 'graphs']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You are given an integer $$$x$$$ of $$$n$$$ di...</td>\n",
       "      <td>['greedy', 'strings']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>You are given a Young diagram.  Given diagram ...</td>\n",
       "      <td>['greedy', 'dynamic programming', 'math']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  John gave Jack a very hard problem. He wrote a...   \n",
       "1  Due to the recent popularity of the Deep learn...   \n",
       "2  Bill is a famous mathematician in BubbleLand. ...   \n",
       "3  The competitors of Bubble Cup X gathered after...   \n",
       "4  John has just bought a new car and is planning...   \n",
       "5  Consider an array A with N elements, all being...   \n",
       "6  The citizens of BubbleLand are celebrating the...   \n",
       "7  This story is happening in a town named Bubble...   \n",
       "8  You are given an integer $$$x$$$ of $$$n$$$ di...   \n",
       "9  You are given a Young diagram.  Given diagram ...   \n",
       "\n",
       "                                         labels  \n",
       "0                                      ['math']  \n",
       "1           ['dynamic programming', 'matrices']  \n",
       "2                         ['greedy', 'sorting']  \n",
       "3  ['shortest path', 'graphs', 'binary search']  \n",
       "4                       ['dynamic programming']  \n",
       "5    ['combinatorics', 'number theory', 'math']  \n",
       "6           ['dynamic programming', 'geometry']  \n",
       "7                           ['trees', 'graphs']  \n",
       "8                         ['greedy', 'strings']  \n",
       "9     ['greedy', 'dynamic programming', 'math']  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./dataset/problems.csv\", usecols=[\"description\", \"labels\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc954335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10912 entries, 0 to 10911\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   description  10912 non-null  object\n",
      " 1   labels       10912 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 170.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# df.shape\n",
    "df.info()\n",
    "# df.duplicated().sum()\n",
    "# df['description'].str.len().plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d00d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math                   3466\n",
      "greedy                 3293\n",
      "data structures        2710\n",
      "dynamic programming    2630\n",
      "graphs                 1950\n",
      "sorting                1488\n",
      "strings                1415\n",
      "binary search          1366\n",
      "trees                  1055\n",
      "number theory           819\n",
      "bit manipulation        785\n",
      "combinatorics           736\n",
      "two pointers            732\n",
      "union find              424\n",
      "geometry                411\n",
      "divide and conquer      334\n",
      "matrices                333\n",
      "shortest path           289\n",
      "game theory             257\n",
      "hashing                 239\n",
      "probabilities           229\n",
      "interactive             210\n",
      "Name: count, dtype: int64\n",
      "總共有 22 種 labels\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "df['labels'] = df['labels'].apply(ast.literal_eval)\n",
    "labels_cnt = [l for lab in df['labels'] for l in lab]\n",
    "label_series = pd.Series(labels_cnt).value_counts()\n",
    "print(label_series)\n",
    "\n",
    "print(\"總共有\", label_series.index.nunique(), \"種 labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b2657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import torch\n",
    "multilabel = MultiLabelBinarizer()\n",
    "labels = multilabel.fit_transform(df[\"labels\"]).astype('float32') # NumPy ndarray # To align label format with model prediction (both are float)\n",
    "texts = df[\"description\"].tolist()\n",
    "\n",
    "co_matrix = np.dot(labels.T, labels)  # labels: shape (N_samples, N_labels)\n",
    "total = np.sum(co_matrix)\n",
    "# 機率矩陣\n",
    "P_ij = co_matrix / total\n",
    "# 邊際機率 P(i)\n",
    "P_i = np.diag(co_matrix) / total  # shape: (n_labels,)\n",
    "# 外積計算 P(i) * P(j)\n",
    "P_i_P_j = np.outer(P_i, P_i)\n",
    "# PMI 計算，加上小常數避免 log(0)\n",
    "PMI = np.log(P_ij / (P_i_P_j + 1e-10) + 1e-10)\n",
    "np.fill_diagonal(PMI, PMI.max())\n",
    "# Normalize PMI to [0, 1] for soft label weight\n",
    "PMI_norm = (PMI - PMI.min()) / (PMI.max() - PMI.min())\n",
    "PMI_tensor = torch.tensor(PMI_norm, dtype=torch.float32)\n",
    "\n",
    "# soft_labels = torch.matmul(torch.tensor(labels), PMI_tensor)\n",
    "# soft_labels = torch.clamp(soft_labels, 0.0, 1.0)\n",
    "# soft_labels = torch.round(soft_labels * 10) / 10\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# label_names = multilabel.classes_\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(PMI, xticklabels=label_names, yticklabels=label_names, cmap='YlGnBu', annot=False)\n",
    "# plt.title(\"Label Co-occurrence Matrix\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0119630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.7014)\n"
     ]
    }
   ],
   "source": [
    "# calculate loss_weights to deal with dataset imbalance\n",
    "label_counts = labels.sum(axis=0)\n",
    "k = 100\n",
    "weights = 1.0 / np.log(label_counts + k)\n",
    "weights = weights / np.max(weights)  # normalize to [0, 1]\n",
    "loss_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "# print(multilabel.classes_)\n",
    "# print(label_counts)\n",
    "print(loss_weights.max())\n",
    "print(loss_weights.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d67abc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/cp-problem-classifier/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b356ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.8922318  0.59647906 0.5165833  0.8321489  0.7325249  0.7068355\n",
      " 0.7786813  0.685588   0.71685344 1.         0.6454635  0.5578946\n",
      " 0.7105639  0.5049038  0.57523435 0.40439364 0.68423986 1.\n",
      " 0.66521287 0.67514175 0.94502926 0.78884274]\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "texts_np = np.array(texts)  \n",
    "labels_np = np.array(labels)\n",
    "\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split( # need  multi-hot vector should be integer\n",
    "        texts_np.reshape(-1, 1), labels_np, test_size=0.2\n",
    "    )\n",
    "\n",
    "# 還原回原本格式\n",
    "train_texts = X_train.ravel().tolist()\n",
    "val_texts = X_val.ravel().tolist()\n",
    "\n",
    "train_labels = torch.matmul(torch.tensor(y_train), PMI_tensor)\n",
    "val_labels = torch.matmul(torch.tensor(y_val), PMI_tensor)\n",
    "train_labels = torch.clamp(train_labels, 0.0, 1.0)\n",
    "val_labels = torch.clamp(val_labels, 0.0, 1.0)\n",
    "\n",
    "print(labels_np[0]) # before \n",
    "print(val_labels.numpy()[0]) # after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2070dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class DistilBertWithSoftLabel(nn.Module):\n",
    "    def __init__(self, num_labels, loss_weights):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction='none')  # No reduction \n",
    "        self.loss_weights = loss_weights\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])  # [CLS] token\n",
    "        \n",
    "        if labels is not None: # label is soft_labels\n",
    "            loss_matrix = self.loss_fn(logits, labels)\n",
    "            # loss = loss_matrix.mean()  # or weighted sum\n",
    "            loss_weights = self.loss_weights.to(logits.device)\n",
    "            loss = (loss_matrix * loss_weights).mean()\n",
    "\n",
    "            return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "def data_collator(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        # 'labels': torch.stack([torch.tensor(x['labels'], dtype=torch.float32) for x in batch])\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6928b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "label_counts = labels.sum(axis=0)\n",
    "model = DistilBertWithSoftLabel(num_labels=len(labels[0]),loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "907ef1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = str(self.texts[idx])\n",
    "    label = self.labels[idx]\n",
    "    # label = torch.tensor(self.labels[idx])\n",
    "    if not isinstance(label, torch.Tensor):\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "    else:\n",
    "        label = label.detach().clone().float()\n",
    "\n",
    "    encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'].squeeze(0),\n",
    "        'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "        'labels': label\n",
    "    }\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6db397ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Label Classification Evaluation Metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score, hamming_loss, roc_curve\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "\n",
    "def find_optimal_thresholds(y_true, y_probs):\n",
    "    thresholds = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        fpr, tpr, th = roc_curve(y_true[:, i], y_probs[:, i])\n",
    "        youdens_j = tpr - fpr\n",
    "        best_th = th[np.argmax(youdens_j)]\n",
    "        thresholds.append(best_th)\n",
    "    print(\"Optimal thresholds:\", thresholds)\n",
    "    return np.array(thresholds)\n",
    "\n",
    "\n",
    "def find_f1_optimal_thresholds(y_true, y_probs):\n",
    "    thresholds = []\n",
    "    y_true = (y_true >= 0.5).astype(int)\n",
    "    for i in range(y_true.shape[1]):\n",
    "        best_f1 = 0\n",
    "        best_th = 0.5\n",
    "        for th in np.linspace(0.05, 0.95, 50):\n",
    "            y_pred_i = (y_probs[:, i] >= th).astype(int)\n",
    "            f1 = f1_score(y_true[:, i], y_pred_i)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_th = th\n",
    "        thresholds.append(best_th)\n",
    "    return np.array(thresholds)\n",
    "\n",
    "\n",
    "def multi_labels_metrics(predictions, labels):\n",
    "  sigmoid = torch.nn.Sigmoid()\n",
    "  # probs = sigmoid(torch.Tensor(predictions))\n",
    "  probs = sigmoid(torch.tensor(predictions)).detach().cpu().numpy()\n",
    "\n",
    "  y_pred = np.zeros(probs.shape)\n",
    "  thresholds = np.maximum(find_optimal_thresholds(labels,probs), 0.05)\n",
    "#   thresholds = np.full(probs.shape[1], 0.3)\n",
    "\n",
    "  y_pred = (probs >= thresholds).astype(int)\n",
    "  y_true = labels\n",
    "  \n",
    "  f1 = f1_score(y_true, y_pred, average = 'macro')\n",
    "  roc_auc = roc_auc_score(y_true, probs, average = 'macro')\n",
    "  hamming = hamming_loss(y_true, y_pred)\n",
    "\n",
    "  metrics = {\n",
    "      \"roc_auc\": roc_auc,\n",
    "      \"hamming_loss\": hamming,\n",
    "      \"f1\": f1\n",
    "  }\n",
    "  return metrics\n",
    "\n",
    "def compute_metrics(p:EvalPrediction):\n",
    "  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "  labels = (p.label_ids > 0.5).astype(int) # p.label_ids\n",
    "  result = multi_labels_metrics(predictions=preds,\n",
    "                                labels=labels)\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb4378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=5,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset = val_dataset,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b9c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='316' max='5470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 316/5470 00:38 < 10:36, 8.10 it/s, Epoch 0.29/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2eb217",
   "metadata": {},
   "source": [
    "## k=100, Threshold - roc optimal , p.label_ids > 0.8 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_roc_auc': 0.6355803896134508,\n",
    " 'eval_hamming_loss': 0.39180860367301046,\n",
    " 'eval_f1': 0.5670835103814952,\n",
    " 'eval_runtime': 22.0219,\n",
    " 'eval_samples_per_second': 99.129,\n",
    " 'eval_steps_per_second': 12.397,\n",
    " 'epoch': 5.0}\n",
    "\n",
    "## k=100, Threshold = 0.3 , p.label_ids > 0.8 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_model_preparation_time': 0.0041,\n",
    " 'eval_roc_auc': 0.6355803896134508,\n",
    " 'eval_hamming_loss': 0.5140132428268022,\n",
    " 'eval_f1': 0.6445478260151899,\n",
    " 'eval_runtime': 21.2506,\n",
    " 'eval_samples_per_second': 102.727,\n",
    " 'eval_steps_per_second': 12.847}\n",
    "\n",
    "## k=100, Threshold - f1 optimal , p.label_ids > 0.5 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_model_preparation_time': 0.0029,\n",
    " 'eval_roc_auc': 0.6209105006225866,\n",
    " 'eval_hamming_loss': 0.2817015783117478,\n",
    " 'eval_f1': 0.8335942488331942,\n",
    " 'eval_runtime': 21.624,\n",
    " 'eval_samples_per_second': 100.953,\n",
    " 'eval_steps_per_second': 12.625}\n",
    "\n",
    "## k=100, Threshold - roc optimal , p.label_ids > 0.5 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_model_preparation_time': 0.0062,\n",
    " 'eval_roc_auc': 0.6209105006225866,\n",
    " 'eval_hamming_loss': 0.3794194811143964,\n",
    " 'eval_f1': 0.7055115285615337,\n",
    " 'eval_runtime': 19.9769,\n",
    " 'eval_samples_per_second': 109.276,\n",
    " 'eval_steps_per_second': 13.666}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89716f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0eb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"distilbert-finetuned-imdb-multi-label\")\n",
    "trainer.save_model(\"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"multi-label-binarizer.pkl\", \"wb\") as f:\n",
    "#   pickle.dump(multilabel, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78239c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Evaluating...\")\n",
    "preds = trainer.predict(val_dataset).predictions\n",
    "pred_binary = (preds > 0.5).astype(int)\n",
    "val_labels_binary = (val_labels > 0.5).int().numpy()\n",
    "print(\"\\nClassification Report:\")\n",
    "label_names = multilabel.classes_\n",
    "print(classification_report(val_labels_binary, pred_binary, target_names=label_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
