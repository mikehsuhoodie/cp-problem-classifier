{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9011ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U accelerate\n",
    "# !pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e3374ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John gave Jack a very hard problem. He wrote a...</td>\n",
       "      <td>['math']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Due to the recent popularity of the Deep learn...</td>\n",
       "      <td>['dynamic programming', 'matrices']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bill is a famous mathematician in BubbleLand. ...</td>\n",
       "      <td>['greedy', 'sorting']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The competitors of Bubble Cup X gathered after...</td>\n",
       "      <td>['shortest path', 'graphs', 'binary search']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John has just bought a new car and is planning...</td>\n",
       "      <td>['dynamic programming']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consider an array A with N elements, all being...</td>\n",
       "      <td>['combinatorics', 'number theory', 'math']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The citizens of BubbleLand are celebrating the...</td>\n",
       "      <td>['dynamic programming', 'geometry']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This story is happening in a town named Bubble...</td>\n",
       "      <td>['trees', 'graphs']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You are given an integer $$$x$$$ of $$$n$$$ di...</td>\n",
       "      <td>['greedy', 'strings']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>You are given a Young diagram.  Given diagram ...</td>\n",
       "      <td>['greedy', 'dynamic programming', 'math']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  John gave Jack a very hard problem. He wrote a...   \n",
       "1  Due to the recent popularity of the Deep learn...   \n",
       "2  Bill is a famous mathematician in BubbleLand. ...   \n",
       "3  The competitors of Bubble Cup X gathered after...   \n",
       "4  John has just bought a new car and is planning...   \n",
       "5  Consider an array A with N elements, all being...   \n",
       "6  The citizens of BubbleLand are celebrating the...   \n",
       "7  This story is happening in a town named Bubble...   \n",
       "8  You are given an integer $$$x$$$ of $$$n$$$ di...   \n",
       "9  You are given a Young diagram.  Given diagram ...   \n",
       "\n",
       "                                         labels  \n",
       "0                                      ['math']  \n",
       "1           ['dynamic programming', 'matrices']  \n",
       "2                         ['greedy', 'sorting']  \n",
       "3  ['shortest path', 'graphs', 'binary search']  \n",
       "4                       ['dynamic programming']  \n",
       "5    ['combinatorics', 'number theory', 'math']  \n",
       "6           ['dynamic programming', 'geometry']  \n",
       "7                           ['trees', 'graphs']  \n",
       "8                         ['greedy', 'strings']  \n",
       "9     ['greedy', 'dynamic programming', 'math']  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./dataset/problems.csv\", usecols=[\"description\", \"labels\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc954335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10912 entries, 0 to 10911\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   description  10912 non-null  object\n",
      " 1   labels       10912 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 170.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# df.shape\n",
    "df.info()\n",
    "# df.duplicated().sum()\n",
    "# df['description'].str.len().plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78d00d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df['labels'] = df['labels'].apply(ast.literal_eval)\n",
    "# labels_cnt = [l for lab in df['labels'] for l in lab]\n",
    "# label_series = pd.Series(labels_cnt).value_counts()\n",
    "# print(label_series)\n",
    "\n",
    "# print(\"總共有\", label_series.index.nunique(), \"種 labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57b2657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import torch\n",
    "multilabel = MultiLabelBinarizer()\n",
    "labels = multilabel.fit_transform(df[\"labels\"]).astype('float32') # NumPy ndarray # To align label format with model prediction (both are float)\n",
    "texts = df[\"description\"].tolist()\n",
    "\n",
    "co_matrix = np.dot(labels.T, labels)  # labels: shape (N_samples, N_labels)\n",
    "total = np.sum(co_matrix)\n",
    "# 機率矩陣\n",
    "P_ij = co_matrix / total\n",
    "# 邊際機率 P(i)\n",
    "P_i = np.diag(co_matrix) / total  # shape: (n_labels,)\n",
    "# 外積計算 P(i) * P(j)\n",
    "P_i_P_j = np.outer(P_i, P_i)\n",
    "# PMI 計算，加上小常數避免 log(0)\n",
    "PMI = np.log(P_ij / (P_i_P_j + 1e-10) + 1e-10)\n",
    "np.fill_diagonal(PMI, PMI.max())\n",
    "# Normalize PMI to [0, 1] for soft label weight\n",
    "PMI_norm = (PMI - PMI.min()) / (PMI.max() - PMI.min())\n",
    "PMI_tensor = torch.tensor(PMI_norm, dtype=torch.float32)\n",
    "\n",
    "# soft_labels = torch.matmul(torch.tensor(labels), PMI_tensor)\n",
    "# soft_labels = torch.clamp(soft_labels, 0.0, 1.0)\n",
    "# soft_labels = torch.round(soft_labels * 10) / 10\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# label_names = multilabel.classes_\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(PMI, xticklabels=label_names, yticklabels=label_names, cmap='YlGnBu', annot=False)\n",
    "# plt.title(\"Label Co-occurrence Matrix\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0119630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.7014)\n"
     ]
    }
   ],
   "source": [
    "label_counts = labels.sum(axis=0)\n",
    "k = 100\n",
    "weights = 1.0 / np.log(label_counts + k)\n",
    "weights = weights / np.max(weights)  # normalize to [0, 1]\n",
    "loss_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "# print(multilabel.classes_)\n",
    "# print(label_counts)\n",
    "print(loss_weights.max())\n",
    "print(loss_weights.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d67abc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b356ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.8922318  0.59647906 0.5165833  0.8321489  0.7325249  0.7068355\n",
      " 0.7786813  0.685588   0.71685344 1.         0.6454635  0.5578946\n",
      " 0.7105639  0.5049038  0.57523435 0.40439364 0.68423986 1.\n",
      " 0.66521287 0.67514175 0.94502926 0.78884274]\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "texts_np = np.array(texts)  \n",
    "labels_np = np.array(labels)\n",
    "# labels_np = soft_labels.numpy()\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split( # need  nulti-hot vector should be integer\n",
    "        texts_np.reshape(-1, 1), labels_np, test_size=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "# 還原回原本格式\n",
    "train_texts = X_train.ravel().tolist()\n",
    "val_texts = X_val.ravel().tolist()\n",
    "# train_labels = y_train\n",
    "# val_labels = y_val\n",
    "\n",
    "train_labels = torch.matmul(torch.tensor(y_train), PMI_tensor)\n",
    "val_labels = torch.matmul(torch.tensor(y_val), PMI_tensor)\n",
    "train_labels = torch.clamp(train_labels, 0.0, 1.0)\n",
    "val_labels = torch.clamp(val_labels, 0.0, 1.0)\n",
    "\n",
    "\n",
    "# 統計出現次數（要轉回 numpy）\n",
    "# train_labels_cnt = train_labels.sum(dim=0).numpy()\n",
    "# val_label_counts = val_labels.sum(dim=0).numpy()\n",
    "\n",
    "# print(\"Train label counts:\", train_labels_cnt)\n",
    "# print(\"Val label counts:\", val_label_counts)\n",
    "print(labels_np[0])\n",
    "print(val_labels.numpy()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4a2070dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class DistilBertWithSoftLabel(nn.Module):\n",
    "    def __init__(self, num_labels, loss_weights):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction='none')  # No reduction \n",
    "        self.loss_weights = loss_weights\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])  # [CLS] token\n",
    "        \n",
    "        if labels is not None: # label is soft_labels\n",
    "            loss_matrix = self.loss_fn(logits, labels)\n",
    "            # loss = loss_matrix.mean()  # or weighted sum\n",
    "            loss_weights = self.loss_weights.to(logits.device)\n",
    "            loss = (loss_matrix * loss_weights).mean()\n",
    "\n",
    "            return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "def data_collator(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        # 'labels': torch.stack([torch.tensor(x['labels'], dtype=torch.float32) for x in batch])\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc9bb38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6928b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels[0]),\n",
    "#                                                             problem_type=\"multi_label_classification\")\n",
    "label_counts = labels.sum(axis=0)\n",
    "model = DistilBertWithSoftLabel(num_labels=len(labels[0]),loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "907ef1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = str(self.texts[idx])\n",
    "    label = self.labels[idx]\n",
    "    # label = torch.tensor(self.labels[idx])\n",
    "    if not isinstance(label, torch.Tensor):\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "    else:\n",
    "        label = label.detach().clone().float()\n",
    "\n",
    "    encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'].squeeze(0),\n",
    "        'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "        'labels': label\n",
    "    }\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6db397ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Label Classification Evaluation Metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score, hamming_loss\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "\n",
    "def multi_labels_metrics(predictions, labels, threshold=0.3):\n",
    "  sigmoid = torch.nn.Sigmoid()\n",
    "  probs = sigmoid(torch.Tensor(predictions))\n",
    "\n",
    "  y_pred = np.zeros(probs.shape)\n",
    "  y_pred[np.where(probs>=threshold)] = 1\n",
    "  y_true = labels\n",
    "  \n",
    "  f1 = f1_score(y_true, y_pred, average = 'macro')\n",
    "  roc_auc = roc_auc_score(y_true, probs, average = 'macro')\n",
    "  hamming = hamming_loss(y_true, y_pred)\n",
    "  # # PMI-aware accuracy\n",
    "  # correct = 0\n",
    "  # total = 0\n",
    "  # N, L = y_true.shape # (num_of_samples,num_of_labels)\n",
    "  # pmi_matrix = pmi_matrix.numpy()  # Convert to numpy if it's a tensor\n",
    "\n",
    "  # for sample_idx in range(N):\n",
    "  #   for label_idx in range(L):\n",
    "  #     if y_true[sample_idx][label_idx] == 1:\n",
    "  #       total += 1\n",
    "  #       if y_pred[sample_idx][label_idx] == 1:\n",
    "  #         correct += 1\n",
    "  #       else:\n",
    "  #         related_preds = np.where((y_pred[sample_idx] == 1) & \n",
    "  #                                  (pmi_matrix[label_idx] >= pmi_threshold))[0]\n",
    "  #         if len(related_preds) > 0:\n",
    "  #             correct += 0.5\n",
    "\n",
    "  # pmi_acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "  metrics = {\n",
    "      \"roc_auc\": roc_auc,\n",
    "      \"hamming_loss\": hamming,\n",
    "      \"f1\": f1\n",
    "      # \"pmi_aware_accuracy\": pmi_acc\n",
    "  }\n",
    "  return metrics\n",
    "\n",
    "def compute_metrics(p:EvalPrediction):\n",
    "  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "  labels = (p.label_ids > 0.5).astype(int) # p.label_ids\n",
    "  result = multi_labels_metrics(predictions=preds,\n",
    "                                labels=labels)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "beb4378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=5,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset = val_dataset,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "364b9c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5460' max='5460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5460/5460 12:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.481600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.485100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.461700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.409400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5460, training_loss=0.45647012899210165, metrics={'train_runtime': 731.6308, 'train_samples_per_second': 59.682, 'train_steps_per_second': 7.463, 'total_flos': 0.0, 'train_loss': 0.45647012899210165, 'epoch': 5.0})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc88ee57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5321438312530518,\n",
       " 'eval_roc_auc': 0.6209178427016014,\n",
       " 'eval_hamming_loss': 0.27911051775209644,\n",
       " 'eval_f1': 0.835518813834735,\n",
       " 'eval_runtime': 22.7607,\n",
       " 'eval_samples_per_second': 95.735,\n",
       " 'eval_steps_per_second': 11.994,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f89716f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary search' 'bit manipulation' 'combinatorics' 'data structures'\n",
      " 'divide and conquer' 'dynamic programming' 'game theory' 'geometry'\n",
      " 'graphs' 'greedy' 'hashing' 'interactive' 'math' 'matrices'\n",
      " 'number theory' 'probabilities' 'shortest path' 'sorting' 'strings'\n",
      " 'trees' 'two pointers' 'union find']\n"
     ]
    }
   ],
   "source": [
    "print(multilabel.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b0eb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"distilbert-finetuned-imdb-multi-label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ffdb6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"multi-label-binarizer.pkl\", \"wb\") as f:\n",
    "#   pickle.dump(multilabel, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e78239c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "      binary search       0.77      0.80      0.78      1597\n",
      "   bit manipulation       0.76      0.76      0.76      1570\n",
      "      combinatorics       0.83      0.73      0.78      1744\n",
      "    data structures       0.78      0.79      0.79      1618\n",
      " divide and conquer       0.79      0.77      0.78      1641\n",
      "dynamic programming       0.77      0.82      0.80      1618\n",
      "        game theory       0.75      0.76      0.76      1537\n",
      "           geometry       0.73      0.69      0.71      1421\n",
      "             graphs       0.79      0.75      0.77      1619\n",
      "             greedy       0.79      0.87      0.83      1673\n",
      "            hashing       0.77      0.77      0.77      1597\n",
      "        interactive       0.70      0.64      0.67      1329\n",
      "               math       0.80      0.86      0.83      1687\n",
      "           matrices       0.74      0.65      0.69      1484\n",
      "      number theory       0.79      0.74      0.77      1628\n",
      "      probabilities       0.71      0.71      0.71      1373\n",
      "      shortest path       0.75      0.69      0.72      1502\n",
      "            sorting       0.76      0.79      0.78      1575\n",
      "            strings       0.73      0.67      0.70      1450\n",
      "              trees       0.78      0.74      0.76      1571\n",
      "       two pointers       0.77      0.79      0.78      1598\n",
      "         union find       0.80      0.75      0.77      1629\n",
      "\n",
      "          micro avg       0.77      0.76      0.76     34461\n",
      "          macro avg       0.77      0.75      0.76     34461\n",
      "       weighted avg       0.77      0.76      0.76     34461\n",
      "        samples avg       0.72      0.75      0.62     34461\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/cp-problem-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Evaluating...\")\n",
    "preds = trainer.predict(val_dataset).predictions\n",
    "pred_binary = (preds > 0.5).astype(int)\n",
    "val_labels_binary = (val_labels > 0.5).int().numpy()\n",
    "print(\"\\nClassification Report:\")\n",
    "label_names = multilabel.classes_\n",
    "print(classification_report(val_labels_binary, pred_binary, target_names=label_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
