{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9011ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U accelerate\n",
    "# !pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3374ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./dataset/problems.csv\", usecols=[\"description\", \"labels\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc954335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape\n",
    "df.info()\n",
    "# df.duplicated().sum()\n",
    "# df['description'].str.len().plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d00d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df['labels'] = df['labels'].apply(ast.literal_eval)\n",
    "labels_cnt = [l for lab in df['labels'] for l in lab]\n",
    "label_series = pd.Series(labels_cnt).value_counts()\n",
    "print(label_series)\n",
    "\n",
    "print(\"總共有\", label_series.index.nunique(), \"種 labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import torch\n",
    "multilabel = MultiLabelBinarizer()\n",
    "labels = multilabel.fit_transform(df[\"labels\"]).astype('float32') # NumPy ndarray # To align label format with model prediction (both are float)\n",
    "texts = df[\"description\"].tolist()\n",
    "\n",
    "co_matrix = np.dot(labels.T, labels)  # labels: shape (N_samples, N_labels)\n",
    "total = np.sum(co_matrix)\n",
    "# 機率矩陣\n",
    "P_ij = co_matrix / total\n",
    "# 邊際機率 P(i)\n",
    "P_i = np.diag(co_matrix) / total  # shape: (n_labels,)\n",
    "# 外積計算 P(i) * P(j)\n",
    "P_i_P_j = np.outer(P_i, P_i)\n",
    "# PMI 計算，加上小常數避免 log(0)\n",
    "PMI = np.log(P_ij / (P_i_P_j + 1e-10) + 1e-10)\n",
    "np.fill_diagonal(PMI, PMI.max())\n",
    "# Normalize PMI to [0, 1] for soft label weight\n",
    "PMI_norm = (PMI - PMI.min()) / (PMI.max() - PMI.min())\n",
    "PMI_tensor = torch.tensor(PMI_norm, dtype=torch.float32)\n",
    "\n",
    "# soft_labels = torch.matmul(torch.tensor(labels), PMI_tensor)\n",
    "# soft_labels = torch.clamp(soft_labels, 0.0, 1.0)\n",
    "# soft_labels = torch.round(soft_labels * 10) / 10\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# label_names = multilabel.classes_\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(PMI, xticklabels=label_names, yticklabels=label_names, cmap='YlGnBu', annot=False)\n",
    "# plt.title(\"Label Co-occurrence Matrix\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss_weights to deal with dataset imbalance\n",
    "label_counts = labels.sum(axis=0)\n",
    "k = 100\n",
    "weights = 1.0 / np.log(label_counts + k)\n",
    "weights = weights / np.max(weights)  # normalize to [0, 1]\n",
    "loss_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "# print(multilabel.classes_)\n",
    "# print(label_counts)\n",
    "print(loss_weights.max())\n",
    "print(loss_weights.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67abc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b356ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "texts_np = np.array(texts)  \n",
    "labels_np = np.array(labels)\n",
    "\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split( # need  multi-hot vector should be integer\n",
    "        texts_np.reshape(-1, 1), labels_np, test_size=0.2\n",
    "    )\n",
    "\n",
    "# 還原回原本格式\n",
    "train_texts = X_train.ravel().tolist()\n",
    "val_texts = X_val.ravel().tolist()\n",
    "\n",
    "train_labels = torch.matmul(torch.tensor(y_train), PMI_tensor)\n",
    "val_labels = torch.matmul(torch.tensor(y_val), PMI_tensor)\n",
    "train_labels = torch.clamp(train_labels, 0.0, 1.0)\n",
    "val_labels = torch.clamp(val_labels, 0.0, 1.0)\n",
    "\n",
    "print(labels_np[0]) # before \n",
    "print(val_labels.numpy()[0]) # after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2070dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class DistilBertWithSoftLabel(nn.Module):\n",
    "    def __init__(self, num_labels, loss_weights):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction='none')  # No reduction \n",
    "        self.loss_weights = loss_weights\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])  # [CLS] token\n",
    "        \n",
    "        if labels is not None: # label is soft_labels\n",
    "            loss_matrix = self.loss_fn(logits, labels)\n",
    "            # loss = loss_matrix.mean()  # or weighted sum\n",
    "            loss_weights = self.loss_weights.to(logits.device)\n",
    "            loss = (loss_matrix * loss_weights).mean()\n",
    "\n",
    "            return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "def data_collator(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        # 'labels': torch.stack([torch.tensor(x['labels'], dtype=torch.float32) for x in batch])\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "label_counts = labels.sum(axis=0)\n",
    "model = DistilBertWithSoftLabel(num_labels=len(labels[0]),loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ef1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = str(self.texts[idx])\n",
    "    label = self.labels[idx]\n",
    "    # label = torch.tensor(self.labels[idx])\n",
    "    if not isinstance(label, torch.Tensor):\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "    else:\n",
    "        label = label.detach().clone().float()\n",
    "\n",
    "    encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'].squeeze(0),\n",
    "        'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "        'labels': label\n",
    "    }\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db397ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Label Classification Evaluation Metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score, hamming_loss, roc_curve\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "\n",
    "def find_optimal_thresholds(y_true, y_probs):\n",
    "    thresholds = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        fpr, tpr, th = roc_curve(y_true[:, i], y_probs[:, i])\n",
    "        youdens_j = tpr - fpr\n",
    "        best_th = th[np.argmax(youdens_j)]\n",
    "        thresholds.append(best_th)\n",
    "    print(\"Optimal thresholds:\", thresholds)\n",
    "    return np.array(thresholds)\n",
    "\n",
    "\n",
    "def find_f1_optimal_thresholds(y_true, y_probs):\n",
    "    thresholds = []\n",
    "    y_true = (y_true >= 0.5).astype(int)\n",
    "    for i in range(y_true.shape[1]):\n",
    "        best_f1 = 0\n",
    "        best_th = 0.5\n",
    "        for th in np.linspace(0.05, 0.95, 50):\n",
    "            y_pred_i = (y_probs[:, i] >= th).astype(int)\n",
    "            f1 = f1_score(y_true[:, i], y_pred_i)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_th = th\n",
    "        thresholds.append(best_th)\n",
    "    return np.array(thresholds)\n",
    "\n",
    "\n",
    "def multi_labels_metrics(predictions, labels):\n",
    "  sigmoid = torch.nn.Sigmoid()\n",
    "  # probs = sigmoid(torch.Tensor(predictions))\n",
    "  probs = sigmoid(torch.tensor(predictions)).detach().cpu().numpy()\n",
    "\n",
    "  y_pred = np.zeros(probs.shape)\n",
    "  thresholds = np.maximum(find_optimal_thresholds(labels,probs), 0.05)\n",
    "#   thresholds = np.full(probs.shape[1], 0.3)\n",
    "\n",
    "  y_pred = (probs >= thresholds).astype(int)\n",
    "  y_true = labels\n",
    "  \n",
    "  f1 = f1_score(y_true, y_pred, average = 'macro')\n",
    "  roc_auc = roc_auc_score(y_true, probs, average = 'macro')\n",
    "  hamming = hamming_loss(y_true, y_pred)\n",
    "\n",
    "  metrics = {\n",
    "      \"roc_auc\": roc_auc,\n",
    "      \"hamming_loss\": hamming,\n",
    "      \"f1\": f1\n",
    "  }\n",
    "  return metrics\n",
    "\n",
    "def compute_metrics(p:EvalPrediction):\n",
    "  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "  labels = (p.label_ids > 0.5).astype(int) # p.label_ids\n",
    "  result = multi_labels_metrics(predictions=preds,\n",
    "                                labels=labels)\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=5,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset = val_dataset,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2eb217",
   "metadata": {},
   "source": [
    "## k=100, Threshold - roc optimal , p.label_ids > 0.8 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_roc_auc': 0.6355803896134508,\n",
    " 'eval_hamming_loss': 0.39180860367301046,\n",
    " 'eval_f1': 0.5670835103814952,\n",
    " 'eval_runtime': 22.0219,\n",
    " 'eval_samples_per_second': 99.129,\n",
    " 'eval_steps_per_second': 12.397,\n",
    " 'epoch': 5.0}\n",
    "\n",
    "## k=100, Threshold = 0.3 , p.label_ids > 0.8 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_model_preparation_time': 0.0041,\n",
    " 'eval_roc_auc': 0.6355803896134508,\n",
    " 'eval_hamming_loss': 0.5140132428268022,\n",
    " 'eval_f1': 0.6445478260151899,\n",
    " 'eval_runtime': 21.2506,\n",
    " 'eval_samples_per_second': 102.727,\n",
    " 'eval_steps_per_second': 12.847}\n",
    "\n",
    "## k=100, Threshold - f1 optimal , p.label_ids > 0.5 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_model_preparation_time': 0.0029,\n",
    " 'eval_roc_auc': 0.6209105006225866,\n",
    " 'eval_hamming_loss': 0.2817015783117478,\n",
    " 'eval_f1': 0.8335942488331942,\n",
    " 'eval_runtime': 21.624,\n",
    " 'eval_samples_per_second': 100.953,\n",
    " 'eval_steps_per_second': 12.625}\n",
    "\n",
    "## k=100, Threshold - roc optimal , p.label_ids > 0.5 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_model_preparation_time': 0.0062,\n",
    " 'eval_roc_auc': 0.6209105006225866,\n",
    " 'eval_hamming_loss': 0.3794194811143964,\n",
    " 'eval_f1': 0.7055115285615337,\n",
    " 'eval_runtime': 19.9769,\n",
    " 'eval_samples_per_second': 109.276,\n",
    " 'eval_steps_per_second': 13.666}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89716f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0eb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"distilbert-finetuned-imdb-multi-label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"multi-label-binarizer.pkl\", \"wb\") as f:\n",
    "#   pickle.dump(multilabel, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78239c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Evaluating...\")\n",
    "preds = trainer.predict(val_dataset).predictions\n",
    "pred_binary = (preds > 0.5).astype(int)\n",
    "val_labels_binary = (val_labels > 0.5).int().numpy()\n",
    "print(\"\\nClassification Report:\")\n",
    "label_names = multilabel.classes_\n",
    "print(classification_report(val_labels_binary, pred_binary, target_names=label_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
