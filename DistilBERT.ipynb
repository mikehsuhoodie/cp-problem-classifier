{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f9011ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U accelerate\n",
    "# !pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4e3374ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John gave Jack a very hard problem. He wrote a...</td>\n",
       "      <td>['math']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Due to the recent popularity of the Deep learn...</td>\n",
       "      <td>['dynamic programming', 'matrices']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bill is a famous mathematician in BubbleLand. ...</td>\n",
       "      <td>['greedy', 'sorting']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The competitors of Bubble Cup X gathered after...</td>\n",
       "      <td>['shortest path', 'graphs', 'binary search']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John has just bought a new car and is planning...</td>\n",
       "      <td>['dynamic programming']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consider an array A with N elements, all being...</td>\n",
       "      <td>['combinatorics', 'number theory', 'math']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The citizens of BubbleLand are celebrating the...</td>\n",
       "      <td>['dynamic programming', 'geometry']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This story is happening in a town named Bubble...</td>\n",
       "      <td>['trees', 'graphs']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You are given an integer $$$x$$$ of $$$n$$$ di...</td>\n",
       "      <td>['greedy', 'strings']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>You are given a Young diagram.  Given diagram ...</td>\n",
       "      <td>['greedy', 'dynamic programming', 'math']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  John gave Jack a very hard problem. He wrote a...   \n",
       "1  Due to the recent popularity of the Deep learn...   \n",
       "2  Bill is a famous mathematician in BubbleLand. ...   \n",
       "3  The competitors of Bubble Cup X gathered after...   \n",
       "4  John has just bought a new car and is planning...   \n",
       "5  Consider an array A with N elements, all being...   \n",
       "6  The citizens of BubbleLand are celebrating the...   \n",
       "7  This story is happening in a town named Bubble...   \n",
       "8  You are given an integer $$$x$$$ of $$$n$$$ di...   \n",
       "9  You are given a Young diagram.  Given diagram ...   \n",
       "\n",
       "                                         labels  \n",
       "0                                      ['math']  \n",
       "1           ['dynamic programming', 'matrices']  \n",
       "2                         ['greedy', 'sorting']  \n",
       "3  ['shortest path', 'graphs', 'binary search']  \n",
       "4                       ['dynamic programming']  \n",
       "5    ['combinatorics', 'number theory', 'math']  \n",
       "6           ['dynamic programming', 'geometry']  \n",
       "7                           ['trees', 'graphs']  \n",
       "8                         ['greedy', 'strings']  \n",
       "9     ['greedy', 'dynamic programming', 'math']  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./dataset/problems.csv\", usecols=[\"description\", \"labels\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dc954335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10912 entries, 0 to 10911\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   description  10912 non-null  object\n",
      " 1   labels       10912 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 170.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# df.shape\n",
    "df.info()\n",
    "# df.duplicated().sum()\n",
    "# df['description'].str.len().plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "78d00d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math                   3466\n",
      "greedy                 3293\n",
      "data structures        2710\n",
      "dynamic programming    2630\n",
      "graphs                 1950\n",
      "sorting                1488\n",
      "strings                1415\n",
      "binary search          1366\n",
      "trees                  1055\n",
      "number theory           819\n",
      "bit manipulation        785\n",
      "combinatorics           736\n",
      "two pointers            732\n",
      "union find              424\n",
      "geometry                411\n",
      "divide and conquer      334\n",
      "matrices                333\n",
      "shortest path           289\n",
      "game theory             257\n",
      "hashing                 239\n",
      "probabilities           229\n",
      "interactive             210\n",
      "Name: count, dtype: int64\n",
      "總共有 22 種 labels\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "df['labels'] = df['labels'].apply(ast.literal_eval)\n",
    "labels_cnt = [l for lab in df['labels'] for l in lab]\n",
    "label_series = pd.Series(labels_cnt).value_counts()\n",
    "print(label_series)\n",
    "\n",
    "print(\"總共有\", label_series.index.nunique(), \"種 labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57b2657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import torch\n",
    "multilabel = MultiLabelBinarizer()\n",
    "labels = multilabel.fit_transform(df[\"labels\"]).astype('float32') # NumPy ndarray # To align label format with model prediction (both are float)\n",
    "texts = df[\"description\"].tolist()\n",
    "\n",
    "co_matrix = np.dot(labels.T, labels)  # labels: shape (N_samples, N_labels)\n",
    "total = np.sum(co_matrix)\n",
    "# 機率矩陣\n",
    "P_ij = co_matrix / total\n",
    "# 邊際機率 P(i)\n",
    "P_i = np.diag(co_matrix) / total  # shape: (n_labels,)\n",
    "# 外積計算 P(i) * P(j)\n",
    "P_i_P_j = np.outer(P_i, P_i)\n",
    "# PMI 計算，加上小常數避免 log(0)\n",
    "PMI = np.log(P_ij / (P_i_P_j + 1e-10) + 1e-10)\n",
    "np.fill_diagonal(PMI, PMI.max())\n",
    "# Normalize PMI to [0, 1] for soft label weight\n",
    "PMI_norm = (PMI - PMI.min()) / (PMI.max() - PMI.min())\n",
    "PMI_tensor = torch.tensor(PMI_norm, dtype=torch.float32)\n",
    "\n",
    "k = 3  # 每個 label 最多擴展 3 個其他 label\n",
    "PMI_topk = torch.zeros_like(PMI_tensor)\n",
    "for i in range(PMI_tensor.shape[0]):\n",
    "    topk_indices = torch.topk(PMI_tensor[i], k=k+1).indices  # +1 保留自己\n",
    "    PMI_topk[i, topk_indices] = PMI_tensor[i, topk_indices]\n",
    "\n",
    "# soft_labels = torch.matmul(torch.tensor(labels), PMI_tensor)\n",
    "# soft_labels = torch.clamp(soft_labels, 0.0, 1.0)\n",
    "# soft_labels = torch.round(soft_labels * 10) / 10\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# label_names = multilabel.classes_\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(PMI, xticklabels=label_names, yticklabels=label_names, cmap='YlGnBu', annot=False)\n",
    "# plt.title(\"Label Co-occurrence Matrix\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0119630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.7014)\n"
     ]
    }
   ],
   "source": [
    "# calculate loss_weights to deal with dataset imbalance\n",
    "label_counts = labels.sum(axis=0)\n",
    "coefficient = 100\n",
    "weights = 1.0 / np.log(label_counts + coefficient)\n",
    "weights = weights / np.max(weights)  # normalize to [0, 1]\n",
    "loss_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "# print(multilabel.classes_)\n",
    "# print(label_counts)\n",
    "print(loss_weights.max())\n",
    "print(loss_weights.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d67abc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0b356ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.10938767 0.1214942  0.1554979  0.08815955 0.10727646 0.11198483\n",
      " 0.12614143 0.1325789  0.08376686 0.1150557  0.09712635 0.11303254\n",
      " 1.         0.10799404 0.16193527 0.1496429  0.06120315 0.09811347\n",
      " 0.07484833 0.07347731 0.09515471 0.08525355]\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "texts_np = np.array(texts)  \n",
    "labels_np = np.array(labels)\n",
    "\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split( # need  multi-hot vector should be integer\n",
    "        texts_np.reshape(-1, 1), labels_np, test_size=0.2\n",
    "    )\n",
    "\n",
    "# 還原回原本格式\n",
    "train_texts = X_train.ravel().tolist()\n",
    "val_texts = X_val.ravel().tolist()\n",
    "\n",
    "# train_labels = torch.matmul(torch.tensor(y_train), PMI_topk)\n",
    "# train_labels = torch.clamp(train_labels, 0.0, 1.0)\n",
    "hard = torch.tensor(y_train, dtype=torch.float32)\n",
    "soft = torch.matmul(hard, PMI_tensor)\n",
    "soft = torch.clamp(soft, 0.0, 1.0)\n",
    "alpha = 0.7 \n",
    "train_labels = alpha * hard + (1 - alpha) * soft\n",
    "\n",
    "val_labels = torch.tensor(y_val, dtype=torch.float32)  # hard label\n",
    "val_labels = torch.clamp(val_labels, 0.0, 1.0)\n",
    "\n",
    "print(y_train[0]) # before \n",
    "print(train_labels.numpy()[0]) # after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4a2070dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "\n",
    "class DistilBertWithSoftLabel(nn.Module):\n",
    "    def __init__(self, num_labels, loss_weights):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction='none')  # No reduction \n",
    "        self.loss_weights = loss_weights\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])  # [CLS] token\n",
    "        \n",
    "        if labels is not None: # label is soft_labels\n",
    "            loss_matrix = self.loss_fn(logits, labels)\n",
    "            # loss = loss_matrix.mean()  # or weighted sum\n",
    "            loss_weights = self.loss_weights.to(logits.device)\n",
    "            loss = (loss_matrix * loss_weights).mean()\n",
    "\n",
    "            return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "def data_collator(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        # 'labels': torch.stack([torch.tensor(x['labels'], dtype=torch.float32) for x in batch])\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6928b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings\n",
    "from transformers import DistilBertTokenizerFast\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(checkpoint)\n",
    "label_counts = labels.sum(axis=0)\n",
    "model = DistilBertWithSoftLabel(num_labels=len(labels[0]),loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "907ef1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = str(self.texts[idx])\n",
    "    label = self.labels[idx]\n",
    "    # label = torch.tensor(self.labels[idx])\n",
    "    if not isinstance(label, torch.Tensor):\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "    else:\n",
    "        label = label.detach().clone().float()\n",
    "\n",
    "    encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'].squeeze(0),\n",
    "        'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "        'labels': label\n",
    "    }\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6db397ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Label Classification Evaluation Metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score, hamming_loss, roc_curve\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "\n",
    "def find_optimal_thresholds(y_true, y_probs):\n",
    "    thresholds = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        fpr, tpr, th = roc_curve(y_true[:, i], y_probs[:, i])\n",
    "        youdens_j = tpr - fpr\n",
    "        best_th = th[np.argmax(youdens_j)]\n",
    "        thresholds.append(best_th)\n",
    "    print(\"Optimal thresholds:\", thresholds)\n",
    "    return np.array(thresholds)\n",
    "\n",
    "\n",
    "def find_f1_optimal_thresholds(y_true, y_probs):\n",
    "    thresholds = []\n",
    "    y_true = (y_true >= 0.5).astype(int)\n",
    "    for i in range(y_true.shape[1]):\n",
    "        best_f1 = 0\n",
    "        best_th = 0.5\n",
    "        for th in np.linspace(0.05, 0.95, 50):\n",
    "            y_pred_i = (y_probs[:, i] >= th).astype(int)\n",
    "            f1 = f1_score(y_true[:, i], y_pred_i)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_th = th\n",
    "        thresholds.append(best_th)\n",
    "    return np.array(thresholds)\n",
    "\n",
    "\n",
    "def multi_labels_metrics(predictions, labels):\n",
    "  sigmoid = torch.nn.Sigmoid()\n",
    "  # probs = sigmoid(torch.Tensor(predictions))\n",
    "  probs = sigmoid(torch.tensor(predictions)).detach().cpu().numpy()\n",
    "\n",
    "  y_pred = np.zeros(probs.shape)\n",
    "  thresholds = np.maximum(find_f1_optimal_thresholds(labels,probs), 0.05)\n",
    "#   thresholds = np.full(probs.shape[1], 0.3)\n",
    "\n",
    "  y_pred = (probs >= thresholds).astype(int)\n",
    "  y_true = labels\n",
    "  \n",
    "  f1 = f1_score(y_true, y_pred, average = 'macro')\n",
    "  roc_auc = roc_auc_score(y_true, probs, average = 'macro')\n",
    "  hamming = hamming_loss(y_true, y_pred)\n",
    "\n",
    "  metrics = {\n",
    "      \"roc_auc\": roc_auc,\n",
    "      \"hamming_loss\": hamming,\n",
    "      \"f1\": f1\n",
    "  }\n",
    "  return metrics\n",
    "\n",
    "def compute_metrics(p:EvalPrediction):\n",
    "  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "  labels = (p.label_ids > 0.5).astype(int) # p.label_ids\n",
    "  result = multi_labels_metrics(predictions=preds,\n",
    "                                labels=labels)\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "beb4378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=5,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset = val_dataset,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "364b9c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5460' max='5460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5460/5460 10:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.477700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.473900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.468900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.465300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.441500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5460, training_loss=0.46454372196407107, metrics={'train_runtime': 605.3743, 'train_samples_per_second': 72.129, 'train_steps_per_second': 9.019, 'total_flos': 0.0, 'train_loss': 0.46454372196407107, 'epoch': 5.0})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fc88ee57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3183152973651886,\n",
       " 'eval_roc_auc': 0.7542329924741993,\n",
       " 'eval_hamming_loss': 0.14028536860110977,\n",
       " 'eval_f1': 0.45215181529330173,\n",
       " 'eval_runtime': 18.2242,\n",
       " 'eval_samples_per_second': 119.566,\n",
       " 'eval_steps_per_second': 14.98,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2eb217",
   "metadata": {},
   "source": [
    "## k=100, Threshold - roc optimal , p.label_ids > 0.8 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_roc_auc': 0.6355803896134508,\n",
    " 'eval_hamming_loss': 0.39180860367301046,\n",
    " 'eval_f1': 0.5670835103814952,\n",
    " 'eval_runtime': 22.0219,\n",
    " 'eval_samples_per_second': 99.129,\n",
    " 'eval_steps_per_second': 12.397,\n",
    " 'epoch': 5.0}\n",
    "\n",
    "## k=100, Threshold = 0.3 , p.label_ids > 0.8 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_model_preparation_time': 0.0041,\n",
    " 'eval_roc_auc': 0.6355803896134508,\n",
    " 'eval_hamming_loss': 0.5140132428268022,\n",
    " 'eval_f1': 0.6445478260151899,\n",
    " 'eval_runtime': 21.2506,\n",
    " 'eval_samples_per_second': 102.727,\n",
    " 'eval_steps_per_second': 12.847}\n",
    "\n",
    "## k=100, Threshold - f1 optimal , p.label_ids > 0.5 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_model_preparation_time': 0.0029,\n",
    " 'eval_roc_auc': 0.6209105006225866,\n",
    " 'eval_hamming_loss': 0.2817015783117478,\n",
    " 'eval_f1': 0.8335942488331942,\n",
    " 'eval_runtime': 21.624,\n",
    " 'eval_samples_per_second': 100.953,\n",
    " 'eval_steps_per_second': 12.625}\n",
    "\n",
    "## k=100, Threshold - roc optimal , p.label_ids > 0.5 as P\n",
    "{'eval_loss': 0.5422307252883911,\n",
    " 'eval_model_preparation_time': 0.0062,\n",
    " 'eval_roc_auc': 0.6209105006225866,\n",
    " 'eval_hamming_loss': 0.3794194811143964,\n",
    " 'eval_f1': 0.7055115285615337,\n",
    " 'eval_runtime': 19.9769,\n",
    " 'eval_samples_per_second': 109.276,\n",
    " 'eval_steps_per_second': 13.666}\n",
    "\n",
    "{'eval_loss': 0.3183152973651886,\n",
    " 'eval_roc_auc': 0.7542329924741993,\n",
    " 'eval_hamming_loss': 0.14028536860110977,\n",
    " 'eval_f1': 0.45215181529330173,\n",
    " 'eval_runtime': 18.2242,\n",
    " 'eval_samples_per_second': 119.566,\n",
    " 'eval_steps_per_second': 14.98,\n",
    " 'epoch': 5.0}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f89716f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binary search' 'bit manipulation' 'combinatorics' 'data structures'\n",
      " 'divide and conquer' 'dynamic programming' 'game theory' 'geometry'\n",
      " 'graphs' 'greedy' 'hashing' 'interactive' 'math' 'matrices'\n",
      " 'number theory' 'probabilities' 'shortest path' 'sorting' 'strings'\n",
      " 'trees' 'two pointers' 'union find']\n"
     ]
    }
   ],
   "source": [
    "print(multilabel.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4b0eb190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./best_model/tokenizer_config.json',\n",
       " './best_model/special_tokens_map.json',\n",
       " './best_model/vocab.txt',\n",
       " './best_model/added_tokens.json',\n",
       " './best_model/tokenizer.json')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.save_model(\"distilbert-finetuned-imdb-multi-label\")\n",
    "# trainer.save_model(\"best_model\")\n",
    "torch.save(model.state_dict(), \"./best_model/model.pt\")\n",
    "torch.save(loss_weights, \"./best_model/weights.pt\")\n",
    "tokenizer.save_pretrained(\"./best_model\")  # tokenizer 可用 Hugging Face 的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ffdb6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"multi-label-binarizer.pkl\", \"wb\") as f:\n",
    "#   pickle.dump(multilabel, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78239c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "      binary search       0.28      0.07      0.12       273\n",
      "   bit manipulation       0.62      0.25      0.35       157\n",
      "      combinatorics       0.43      0.06      0.10       151\n",
      "    data structures       0.55      0.29      0.38       542\n",
      " divide and conquer       0.00      0.00      0.00        71\n",
      "dynamic programming       0.37      0.24      0.29       526\n",
      "        game theory       0.66      0.53      0.59        51\n",
      "           geometry       0.64      0.52      0.58        82\n",
      "             graphs       0.63      0.53      0.58       390\n",
      "             greedy       0.50      0.47      0.48       659\n",
      "            hashing       0.00      0.00      0.00        50\n",
      "        interactive       1.00      0.88      0.94        42\n",
      "               math       0.58      0.48      0.53       693\n",
      "           matrices       0.93      0.40      0.56        67\n",
      "      number theory       0.69      0.37      0.48       166\n",
      "      probabilities       0.60      0.26      0.36        47\n",
      "      shortest path       0.27      0.07      0.11        58\n",
      "            sorting       0.51      0.17      0.26       298\n",
      "            strings       0.80      0.67      0.73       283\n",
      "              trees       0.84      0.61      0.71       211\n",
      "       two pointers       0.00      0.00      0.00       146\n",
      "         union find       0.12      0.01      0.02        85\n",
      "\n",
      "          micro avg       0.57      0.36      0.44      5048\n",
      "          macro avg       0.50      0.31      0.37      5048\n",
      "       weighted avg       0.52      0.36      0.41      5048\n",
      "        samples avg       0.50      0.38      0.40      5048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/cp-problem-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mike/cp-problem-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Evaluating...\")\n",
    "preds = trainer.predict(val_dataset).predictions\n",
    "pred_binary = (preds > 0.5).astype(int)\n",
    "val_labels_binary = (val_labels > 0.5).int().numpy()\n",
    "print(\"\\nClassification Report:\")\n",
    "label_names = multilabel.classes_\n",
    "print(classification_report(val_labels_binary, pred_binary, target_names=label_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
